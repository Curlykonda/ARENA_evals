{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3.3] Running evals with the inspect library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ Introduction to Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Understand how **solvers** and **scorers** in the inspect framework function\n",
    "- Familiarise yourself with **plans**, which ultimately are just a list of solver functions, and then a scorer function.\n",
    "- Understand how inspect uses **record_to_sample** to modify json datasets into evaluation datasets.\n",
    "- Know the basics of how all of these classes come together to form a **Task** object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Literal\n",
    "from inspect_ai import Task, eval, task\n",
    "from inspect_ai.log import read_eval_log\n",
    "from utils import omit\n",
    "from inspect_ai.dataset import FieldSpec, json_dataset, Sample, example_dataset\n",
    "from inspect_ai.solver._multiple_choice import Solver, solver, Choices, TaskState, answer_options\n",
    "from inspect_ai.solver._critique import DEFAULT_CRITIQUE_TEMPLATE, DEFAULT_CRITIQUE_COMPLETION_TEMPLATE\n",
    "from inspect_ai.scorer import (model_graded_fact, match, answer, scorer)\n",
    "from inspect_ai.scorer._metrics import (accuracy)\n",
    "from inspect_ai.solver import ( chain_of_thought, generate, self_critique,system_message, Generate )\n",
    "from inspect_ai.model import ChatMessageUser, ChatMessageSystem, ChatMessageAssistant, get_model\n",
    "import random\n",
    "import jaxtyping \n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect\n",
    "Inspect (or inspect_ai) is a library written by the UK AISI in order to streamline model evaluations. Inspect provides the following in order to make writing evals easier:\n",
    "\n",
    "- It automates the creation of logging files, and stores a lot of useful information about the evals that we run for us.\n",
    "\n",
    "- It provides nice \"scoring\" functions (and some of its automated solver functions also come in handy, but mostly we will write our own).\n",
    "\n",
    "- It provides a nice layout to view the results of our evals, so we don't have to look directly at model outputs (which can be very gross, as we will find out when we work with agents).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Overview of Inspect\n",
    "\n",
    "\n",
    "The main object of inspect is a `Task` object, which will contain all of the information about the eval we'll run on the model, including:\n",
    "\n",
    "- The dataset we will evaluate the model on. This is essentially a list of `Sample` objects, which is how the inspect library stores individual questions. We will write a function to turn our list of questions formatted as a json file into a list of questions formatted as `Sample` objects.\n",
    "\n",
    "- The `plan` that the evaluation will proceed along. This is a list of functions which inspect calls `Solver` functions. `Solver` functions can modify the evaluation questions; get the model to generate a response; or do both at the same time. A typical collection of solvers to form `plan` list might look like:\n",
    "\n",
    "    - A `chain_of_thought` function which will add an indication that the model should use chain of thought before answering.\n",
    "\n",
    "    - A `generate` function that calls the LLM API to generate a response to the question (now also with the chain of thought prompt).\n",
    "\n",
    "    - A `self_critique` function that takes all of the output so far, and attaches a prompt asking the model to critique its own output.\n",
    "\n",
    "    - Another `generate` call which calls the LLM API to generate an output in response to the `self_critique` prompt.\n",
    "\n",
    "- The final component of a `Task` object is a `scorer` function, which tells the inspect library how it should evaluate the model's output.\n",
    "\n",
    "![image file](Inspect%20Big%20Picture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "A dataset is built out of `Sample` objects. We'll have one `Sample` object for each question in our evaluation. They contain basically the exact same information as we've constructed for each question in our list, but renamed and reformatted so that they fit in with the way the rest of the inspect library is built. This means that turning our list of questions in json format into a `Dataset` of `Sample` objects just requires us to handle the field-mapping. The components of the `Sample` object are:\n",
    "\n",
    "- The `input` to the model. This is a `ChatMessage` list of dictionaries which contains the content which we will feed to the model during the eval consisting of:\n",
    "    - A system message\n",
    "\n",
    "    - A user message (this is the most necessary part of all)\n",
    "\n",
    "- A `target` which represents the \"correct\" answer (or `answer_matching_behavior` in our context).\n",
    "\n",
    "<details> \n",
    "<summary>Aside: Longer ChatMessage lists</summary>\n",
    "<br>\n",
    "For more complicated evals, we're able to do an arbitrary length list of ChatMessages including: \n",
    "<ul> \n",
    "<li>Multiple user messages and system messages.</li>\n",
    "&nbsp;\n",
    "<li>\"Assistant messages\" that the model will believe it has produced in response to user and system messages. However we can write these ourselves to trick the model.</li>\n",
    "&nbsp;\n",
    "<li>\"Tool call messages\" which can mimic the model's interaction with any tools that we give it. We'll learn more about tools in the agent evals section.</li>\n",
    "</ul>\n",
    "</details>\n",
    "\n",
    "Since we are constructing a multiple choice evaluation, we will also need to feed it `choices`. Then it also takes in `metadata`, which can consist of any data we might want to store related to the question in our logs, for example:\n",
    "\n",
    "- If there are multiple categories of question, and we want to be able to plot results by category.\n",
    "\n",
    "- If we are going to conduct the evaluation with/without a system prompt\n",
    "\n",
    "- Any other information about questions that we want to be easily accessible from the log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Write record_to_sample function\n",
    "\n",
    "```c\n",
    "Difficulty: üî¥‚ö™‚ö™‚ö™‚ö™\n",
    "Importance: üîµüîµüîµ‚ö™‚ö™\n",
    "\n",
    "You should spend up to 5-10 minutes on this exercise.\n",
    "```\n",
    "\n",
    "This function will take in a single element of your list of json objects, and returns a Sample object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_to_sample(record : dict) -> Sample:\n",
    "    return Sample(\n",
    "        input = [\n",
    "            {\n",
    "                'role' : \"system\",\n",
    "                \"content\" : record[\"system\"]\n",
    "            },\n",
    "            {\n",
    "                'role' : \"user\",\n",
    "                \"content\" : record[\"question\"]\n",
    "            }\n",
    "        ],\n",
    "        target = record[\"answer_matching_behavior\"],\n",
    "        choices =  list(record[\"answers\"].values()),\n",
    "        metadata = {\"label\" : record[\"label\"], \"system_prompt\" : True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the `json_dataset()` function to apply this function to the list of questions in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = json_dataset(\"C:\\\\Users\\\\styme\\\\OneDrive\\\\Documents\\\\AI STUFF\\\\Model Written Evals\\\\Code Replication\\\\ARENA_evals\\\\day1-3_dataset_generation\\\\datasets\\\\2c-generated-final.json\", record_to_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solvers\n",
    "\n",
    "Solvers are functions which outline how the eval will be presented to the model. They can be used for:\n",
    "\n",
    "- Providing a system prompt.\n",
    " \n",
    "- Providing the evaluation question.\n",
    "\n",
    "- Prompt engineering (e.g. chain of thought, asking the model to output in certain formats, telling the model how to act when answering).\n",
    "\n",
    "- Self-critique (where the model criticises its prior answer to see if it arrives at a different conclusion).\n",
    "\n",
    "- Providing fake assistant messages to the model (useful if you want to conduct an eval in the midst of an ongoing conversation with a model).\n",
    "\n",
    "Some built-in solvers which accomplish these are:\n",
    "\n",
    "- `generate()` which is the most fundamental solver function. This just calls the LLM API to generate a response to the evaluation question.\n",
    "\n",
    "- `system_message()` which adds a system message to the list of ChatMessages.\n",
    "\n",
    "- `prompt_template()` which modifies the user prompt by substituting the current prompt into the `{prompt}` placeholder within the specified template. Can also substitute variables defined in the `metadata` of any Sample object.\n",
    "\n",
    "- `chain_of_thought()` which modifies the user prompt by substituting the current prompt into the `{prompt}` placeholder in order to add a standard chain_of_thought template. By default, this template is:\n",
    "\n",
    "    ```python\n",
    "    DEFAULT_COT_TEMPLATE = r\"\"\"\n",
    "    {prompt}\n",
    "\n",
    "    Before answering, reason in a step-by-step manner as to get the right answer. Provide your answer at the end on its own line in the form \"ANSWER: $ANSWER\" (without quotes) where $ANSWER is the answer to the question.\n",
    "    \"\"\"\n",
    "    ```\n",
    "\n",
    "- `multiple_choice()` which presents a multiple choice question (with a single correct answer) in the format:\n",
    "    ```python\n",
    "    SINGLE_ANSWER_TEMPLATE = r\"\"\"\n",
    "    Answer the following multiple choice question. The entire content of your response should be of the following format: 'ANSWER: $LETTER' (without quotes) where LETTER is one of {letters}.\n",
    "\n",
    "    {question}\n",
    "\n",
    "    {choices}\n",
    "    \"\"\".strip()\n",
    "    ```\n",
    "    where `{choices}` are formatted as ```\"A) Choice 1\\nB) Choice 2\\nC) Choice 3\"```. This solver then automatically calls generate to call the LLM API for a response to the multiple choice question.\n",
    "\n",
    "We'll design our own solvers in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorers\n",
    "\n",
    "Scorers are what the inspect library uses to evaluate model output. We use them to determine whether the model's answer is the `target` answer. From the [inspect documentation](https://inspect.ai-safety-institute.org.uk/scorers.html):\n",
    "\n",
    "\n",
    "><br>\n",
    ">Scorers generally take one of the following forms:\n",
    ">\n",
    ">1. Extracting a specific answer out of a model‚Äôs completion output using a variety of heuristics.\n",
    ">\n",
    ">2. Applying a text similarity algorithm to see if the model‚Äôs completion is close to what is set out in the `target`.\n",
    ">\n",
    ">3. Using another model to assess whether the model‚Äôs completion satisfies a description of the ideal answer in `target`.\n",
    ">\n",
    ">4. Using another rubric entirely (e.g. did the model produce a valid version of a file format, etc.)\n",
    ">\n",
    "><br>\n",
    "\n",
    "Since we are writing a multiple choice benchmark, the scorer we will generally use is called `match()`. This determines whether the `target` appears at the end of the model's output (or the start, but by default it will look at the end). The code for this scorer function can be found [in the inspect_ai github](https://github.com/UKGovernmentBEIS/inspect_ai/blob/c59ce86b9785495338990d84897ccb69d46610ac/src/inspect_ai/scorer/_match.py). \n",
    "\n",
    "<details><summary>Aside: some other scorer functions:</summary>\n",
    "<br>\n",
    "Some other scorer functions that would be useful in different evals are:\n",
    "\n",
    "- `includes()` which determines whether the target appears anywhere in the model output.\n",
    "\n",
    "- `pattern()` which extracts the answer from the model output using a regular expression.\n",
    "\n",
    "- `answer()` which is a scorer for model output when the output is preceded with \"ANSWER:\".\n",
    "\n",
    "- `model_graded_qa()` which has another model assess whether the output is a correct answer based on guidance in `target`. This is especially useful for evals where the answer is more open-ended. Depending on what sort of questions you're asking here, you may want to change the prompt template that is fed to the grader model.\n",
    "<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Task Object\n",
    "\n",
    "The dataset, solvers, and scorers come together in a Task object. This contains everything we need to run the evaluation, and we will be able to call Inspect's `eval` function, choose a model to evaluate, and Inspect will take care of the rest. For example, a Task object might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.dataset import example_dataset\n",
    "\n",
    "@task\n",
    "def theory_of_mind():\n",
    "    return Task(\n",
    "        dataset=example_dataset(\"theory_of_mind\"),\n",
    "        plan=[\n",
    "          chain_of_thought(), \n",
    "          generate(), \n",
    "          self_critique(model = \"openai/gpt-3.5-turbo\")\n",
    "        ],\n",
    "        scorer=model_graded_fact(model = \"openai/gpt-3.5-turbo\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what it looks like to run this example eval through inspect. We'll only run it for 10 samples on gpt-3.5-turbo to minimise the number of OpenAI credits we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44088f7a38d4132a7ec7b4310068d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log = eval(theory_of_mind(),\n",
    "              model = \"openai/gpt-3.5-turbo\",\n",
    "              limit=10,\n",
    "              log_dir=\"./gitignore/logs\" # TODO: Figure out gitignore stuff from Sunischal's advice\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can view the results of the log in Inspect's log viewer. To do this, run the code below. It will locally host a display of the results of the example evaluation at http://localhost:7575. You need to modify \"`your_log_name`\" in the --log-dir argument below so that it matches the log name that was output above (which is dependent on the date and time).\n",
    "\n",
    "<details><summary>Aside: Log names</summary> I'm fairly confident (but not 100%) that when Inspect accesses logs, it utilises the name, date, and time information as a part of the way of accessing and presenting the logging data. Therefore it seems that there is no easy way to rename log files to make them easier to access (I tried it, and Inspect didn't let me open them). If someone works out how to change the names of logs then let me know. </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspect view running at http://localhost:7575/\n"
     ]
    }
   ],
   "source": [
    "!inspect view --log-dir \"./gitignore/logs/2024-08-19T15-20-15+01-00_theory-of-mind_UCqYeegADxYZBDbG8FLZhJ.json\" --port 7575"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Writing Solvers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the utility of Inspect, it is lacking some solvers which we'll want to use, and some solvers which it gives to us are utilised in ways that won't really align with the way that we're running our evaluations. For this reason, we'll write our own solvers.\n",
    "\n",
    "Solvers modify the `TaskState`. This is how Inspect stores the state of each question. There is a different `TaskState` class initialised for every new question during our eval. It contains things like the initial question (stored as a string under state.user_prompt.text), and the ChatMessageHistory (stored as a list under state.messages).\n",
    "\n",
    "Solvers have the potential to take quite a long time to run. For example, the `self_critique` solver needs to wait for the model to generate a response before it can modify the `TaskState` with the criticism. For this reason, Inspect calls solvers using the `await` key word. So when we define a solver, we need to define the modification of the TaskState as its own asynchronous `solve` function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Write a simple solver function\n",
    "```c\n",
    "Difficulty: üî¥üî¥‚ö™‚ö™‚ö™\n",
    "Importance: üîµüîµüîµüîµ‚ö™\n",
    "\n",
    "You should spend up to 10-15 minutes on this exercise.\n",
    "```\n",
    "\n",
    "Write a solver function that checks if `state` has a `user_prompt`, and modifies the `user_prompt` according to a template string which contains `\"{question}\"` ‚Äî where the original `user_prompt` will get placed in the template.\n",
    "\n",
    "You might find Inspect's [solver documentation](https://inspect.ai-safety-institute.org.uk/solvers.html), or even their [github source code](https://github.com/UKGovernmentBEIS/inspect_ai/tree/0d9f9c8d5b472ec4f247763fe301a5be5839a607/src/inspect_ai/solver) useful for this exercise (and the following exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prompt_template(template : str, **params : dict) -> Solver:\n",
    "    async def solve(state : TaskState, generate: Generate) -> TaskState:\n",
    "        if state.user_prompt:\n",
    "            state.user_prompt.text = template.format(\n",
    "                prompt=state.user_prompt.text, **params\n",
    "            )\n",
    "        return state\n",
    "\n",
    "    return solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Write other solver functions\n",
    "```c\n",
    "Difficulty: üî¥üî¥üî¥‚ö™‚ö™\n",
    "Importance: üîµüîµüîµ‚ö™‚ö™\n",
    "\n",
    "You should spend up to 20-25 mins on this exercise.\n",
    "```\n",
    "\n",
    "Now write a greater variety of solver functions that we can use in our evaluations of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='hello' source=None role='system' tool=None\n"
     ]
    }
   ],
   "source": [
    "ChatMessageTemplate = multiple_choice_2_template = {\"role\" : \"user\",\n",
    "                              \"content\" : \"Now provide your conclusion in the form ANSWER: $LETTER where $LETTER is one of A, B. Only output in this format.\"}\n",
    "print(ChatMessageSystem(content = \"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@solver\n",
    "def multiple_choice_format(template : str, **params : dict) -> Solver:\n",
    "    '''\n",
    "    This modifies the initial prompt to be in the format of a multiple choice question. Make sure that {question} and {choices} are in the template string, so that you can format those parts of the prompt.\n",
    "    '''\n",
    "    assert r\"{choices}\" in template and r\"{question}\" in template,r\"ERROR: The template must contain {question} or {choices}.\"\n",
    "\n",
    "    async def solve(state: TaskState, generate: Generate) -> TaskState:\n",
    "        if state.user_prompt:\n",
    "            state.user_prompt.text = template.format(\n",
    "                question=state.user_prompt.text,\n",
    "                choices=answer_options(state.choices),\n",
    "                **params,\n",
    "            )\n",
    "        return state\n",
    "    \n",
    "    return solve\n",
    "\n",
    "@solver\n",
    "def choose_multiple_choice(ChatMessageTemplate : dict, **params : dict) -> Solver:\n",
    "    '''\n",
    "    This modifies the ChatMessageHistory to add a message at the end indicating that the model should make a choice out of the options provided. \n",
    "    \n",
    "    Make sure the model outputs so that its response can be extracted by the \"match\" solver. You will have to use Inspect's ChatMessageUser class for this.\n",
    "    '''\n",
    "    async def solve(state: TaskState, generate: Generate) -> TaskState:\n",
    "        if state.messages:\n",
    "            state.messages.append(ChatMessageUser(**ChatMessageTemplate))\n",
    "        return state\n",
    "    return solve\n",
    "\n",
    "@solver\n",
    "def system_message(system_message : str, **params : dict) -> Solver:\n",
    "    '''\n",
    "    This solver is used to provide a fixed system prompt to the model.\n",
    "\n",
    "    This is built into Inspect, so if you're happy to use their version without any mofifications, you can just use Inspect's system_message solver.\n",
    "    '''\n",
    "    def insert_system_message(messages : list, message: ChatMessageSystem) -> None:\n",
    "        lastSystemMessageIndex = -1\n",
    "        for i in list(reversed(range(0,len(messages)))):\n",
    "            if isinstance(messages[i],ChatMessageSystem):\n",
    "                lastSystemMessageIndex = i\n",
    "                break\n",
    "        messages.insert(lastSystemMessageIndex+1,message)\n",
    "    async def solve(state : TaskState, generate : Generate) -> TaskState:\n",
    "        insert_system_message(state.messages, ChatMessageSystem(content = system_message))\n",
    "        return state\n",
    "    return solve\n",
    "\n",
    "@solver\n",
    "def self_critique(model : str, critique_template : str, critique_completion_template : str, **params) -> Solver:\n",
    "    '''\n",
    "    This solver is used get the model to evaluate its own response to the task. Then appends its criticism as a user message. \n",
    "    \n",
    "    This is built into Inspect, so if you're happy to make use their version without any modifications, you can just use Inspect's self_critique solver.\n",
    "    '''\n",
    "    model = get_model(model)\n",
    "    async def solve(state : TaskState, generate: Generate) -> TaskState:\n",
    "        metadata = omit(state.metadata, [\"question\", \"completion\", \"critique\"])\n",
    "        critique = await model.generate(\n",
    "            critique_template.format(\n",
    "                question=state.input_text,\n",
    "                completion=state.output.completion,\n",
    "                **metadata,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        state.messages.append(\n",
    "            ChatMessageUser(\n",
    "                content = completion_template.format(\n",
    "                    question = state.input_text,\n",
    "                    completion = state.output.completion,\n",
    "                    critique = critique.completion,\n",
    "                    **metadata,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return await generate(state)\n",
    "    return solve\n",
    "\n",
    "# Now you should be able to write any other solvers that might be useful for your specific evaluation task (if you can't already use any of the existing solvers to accomplish the evaluation you want to conduct). For example:\n",
    "    #\n",
    "    # A \"language\" template, which tells the model to respond in diffeent languages (you could even get the model to provide the necessary translation first).\n",
    "    #\n",
    "    # \n",
    "    #\n",
    "    # A solver which provides \"fake\" reasoning by the model where it has already reacted in certain ways, to see if this conditions the model response in different ways. You would have to use the ChatMessageUser and ChatMessageAssistant Inspect classes for this\n",
    "    #\n",
    "    #\n",
    "    #   \n",
    "    #  maybe.\n",
    "    #\n",
    "#  Do so here!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Writing Tasks and evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Benchmarking. May need to write different solvers/definitely need to write different dataset generation procedures to handle system prompts.\n",
    "- Writing then running the overall benchmarking task\n",
    "- Writing then running the overall evaluation task procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can finalize our evaluation task, we'll need to modify our dataset generation from earlier. The issue with the function we wrote is twofold:\n",
    "\n",
    "- First of all, we didn't shuffle the answers. Your dataset may have been generated pre-shuffled, but if not, then you ought to be aware that the model has a bias towards the first answer. See for example [Narayanan & Kapoor: Does ChatGPT have a Liberal Bias?](https://www.aisnakeoil.com/p/does-chatgpt-have-a-liberal-bias) To mitigate this confounder, we should shuffle the answers around when loading in the dataset.\n",
    "\n",
    "<details><summary>Aside: Inspect's shuffle function</summary> \n",
    "\n",
    "Inspect comes with a shuffle function for multiple choice questions. However, they then automatically \"unshuffle\" the responses back to how they are presented in your dataset. \n",
    "\n",
    "I'm sure there are use cases for this, but in our case its often unnecessarily confusing to see the model reasoning about e.g. how much it thinks option B is the correct answer, and then Inspect telling you the model selected \"option A\" since its selection choice gets unshuffled but its reasoning doesn't.\n",
    "\n",
    "For this reason, and because it's not very hard to do, we will just build our own shuffler that doesn't do any unshuffling.</details>\n",
    "\n",
    "- Secondly, this function can only load questions with the corresponding system prompt. It is often useful to load the dataset without a system prompt (for example, this can be useful for benchmarking, or even when running an evaluation, we may want to see how the model reacts to our evaluation with a more generic \"You are a harmless, helpful, honest AI Chat Assistant\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Write record_to_sample function with shuffling\n",
    "```c\n",
    "Difficulty: üî¥üî¥‚ö™‚ö™‚ö™\n",
    "Importance: üîµüîµ‚ö™‚ö™‚ö™\n",
    "```\n",
    "\n",
    "Take the record_to_sample function you wrote in section 1, and modify it so that it randomly shuffles the order in which answers are presented. Remember to correspondingly shuffle the `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_to_sample_shuffle(record : dict, shuffle : bool) -> Sample:\n",
    "    shuffle = random.choice([True,False])\n",
    "    return Sample(\n",
    "        input = [\n",
    "                    {\n",
    "                        \"role\" : \"system\",\n",
    "                        \"content\" : record[\"system\"] if system_prompt == True else \"You are a harmless, helpful, and honest AI chat assistant.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\" : \"user\", \n",
    "                        \"content\" : record[\"question\"] \n",
    "                    }\n",
    "                ],\n",
    "        target = record[\"answer_not_matching_behavior\"][0] if shuffle else record[\"answer_matching_behavior\"][0],\n",
    "        choices = list(reversed(record[\"answers\"].values())) if shuffle else list(record[\"answers\"].values()),\n",
    "        metadata = {\"label\" : record[\"label\"], \"system_prompt\" : True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Write record_to_sample function to handle system prompts differently\n",
    "```c\n",
    "Difficulty: üî¥üî¥‚ö™‚ö™‚ö™\n",
    "Importance: üîµ‚ö™‚ö™‚ö™‚ö™\n",
    "\n",
    "```\n",
    "\n",
    "There are two reasons we might not want the system prompt provided to the model in the usual way. Either:\n",
    "- We might want the model not to have the context of the system prompt, and give it a different (fixed) context entirely (which we can do via the system_prompt solver we wrote in the section 3).\n",
    "\n",
    "- During benchmarking (NEED TO EXPLAIN BENCHMARKING SOMEWHERE HERE PROBABLY), we might not want the model to provide an answer with the system prompt as its actual system prompt, but we still want to provide this system prompt context somewhere in the user prompt.\n",
    "\n",
    "Write a version of the `record_to_sample` function which handles system prompts in each of these two ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_to_sample_no_system_prompt(record : dict, shuffle : bool) -> Sample:\n",
    "    shuffle = random.choice([True,False])\n",
    "    return Sample(\n",
    "        input = [\n",
    "            {\n",
    "                'role' : \"user\",\n",
    "                \"content\" : record[\"question\"]\n",
    "            }\n",
    "        ],\n",
    "        target = record[\"answer_not_matching_behavior\"][0] if shuffle else record[\"answer_matching_behavior\"][0],\n",
    "        choices = list(reversed(record[\"answers\"].values())) if shuffle else list(record[\"answers\"].values()),\n",
    "        metadata = {\"label\" : record[\"label\"], \"system_prompt\" : False}\n",
    "    )\n",
    "\n",
    "def record_to_sample_system_prompt_as_context(record : dict, shuffle : bool) -> Sample:\n",
    "    shuffle = random.choice([True,False])\n",
    "    return Sample(\n",
    "        input = [\n",
    "            {\n",
    "                'role' : \"user\",\n",
    "                \"content\" : \"Context:\" + record[\"system\"] + \"\\n\\n\" + record[\"question\"]\n",
    "            },\n",
    "        ],\n",
    "        target = record[\"answer_not_matching_behavior\"][0] if shuffle else record[\"answer_matching_behavior\"][0],\n",
    "        choices = list(reversed(record[\"answers\"].values())) if shuffle else list(record[\"answers\"].values()),\n",
    "        metadata = {\"label\" : record[\"label\"], \"system_prompt\" : False}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `json_dataset` can only take a function which takes one argument `record`, we need to wrap any `record_to_sample` functions which take optional arguments (like `shuffle`) so that the wrapper takes just one function. This is done in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_to_sample(system, system_prompt_as_context, shuffle):\n",
    "    assert not system and not system_prompt_as_context, \"ERROR: You can only set one of system or system_prompt_as_context to be True.\"\n",
    "    def wrapper(record):\n",
    "        if system:\n",
    "            return record_to_sample(record, shuffle)\n",
    "        elif system_prompt_as_context:\n",
    "            return record_to_sample_system_prompt_as_context(record, shuffle)\n",
    "        else:\n",
    "            return record_to_sample_no_system_prompt(record, shuffle)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load our dataset in using any one of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_system = json_dataset(\"your/path/to/eval_dataset.json\", record_to_sample(system = True, system_prompt_as_context = False, shuffle = True))\n",
    "eval_dataset_no_system = json_dataset(\"your/path/to/eval_dataset.json\", record_to_sample(system = False, system_prompt_as_context = False, shuffle = True))\n",
    "eval_dataset_system_as_context = json_dataset(\"your/path/to/eval_dataset.json\", record_to_sample(system = False, system_prompt_as_context = True, shuffle = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Writing Prompts and Building out your eval task\n",
    "```c\n",
    "Difficulty: üî¥üî¥‚ö™‚ö™‚ö™\n",
    "Importance: üîµ‚ö™‚ö™‚ö™‚ö™\n",
    "\n",
    "You should spend\n",
    "```\n",
    "\n",
    "Now we want to build out the tasks we'll ultimately test the model on.\n",
    "\n",
    "A few things:\n",
    "\n",
    "- Here you really need to sort out prompts\n",
    "- You need to figure out how to benchmarking\n",
    "    - You need to figure out how you're going to deal with Claude being a wimp (well done Anthropic, I guess).\n",
    "- You need to hammer out what evals you want to conduct, and in what way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Dealing with logs and plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
