{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3.3] Running evals with the inspect library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1️⃣ Introduction to Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Understand how **solvers** and **scorers** in the inspect framework function\n",
    "- Familiarise yourself with **plans**, which ultimately are just a list of solver functions, and then a scorer function.\n",
    "- Understand how inspect uses **record_to_sample** to modify json datasets into evaluation datasets.\n",
    "- Know the basics of how all of these classes come together to form a **Task** object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'stderr' from 'inspect_ai.scorer._metrics' (c:\\Users\\styme\\anaconda3\\envs\\arena\\lib\\site-packages\\inspect_ai\\scorer\\_metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiple_choice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Solver, solver, Choices, TaskState, answer_options\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (model_graded_fact, match, answer, scorer)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (accuracy, stderr)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ( chain_of_thought, generate, self_critique,system_message, multiple_choice, Generate )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatMessageUser, ChatMessageSystem, ChatMessageAssistant\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'stderr' from 'inspect_ai.scorer._metrics' (c:\\Users\\styme\\anaconda3\\envs\\arena\\lib\\site-packages\\inspect_ai\\scorer\\_metrics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from inspect_ai import Task, eval, task\n",
    "from inspect_ai.log import read_eval_log\n",
    "from inspect_ai.dataset import FieldSpec, json_dataset, Sample\n",
    "from inspect_ai.solver._multiple_choice import Solver, solver, Choices, TaskState, answer_options\n",
    "from inspect_ai.scorer import (model_graded_fact, match, answer, scorer)\n",
    "from inspect_ai.scorer._metrics import (accuracy, stderr)\n",
    "from inspect_ai.solver import ( chain_of_thought, generate, self_critique,system_message, multiple_choice, Generate )\n",
    "from inspect_ai.model import ChatMessageUser, ChatMessageSystem, ChatMessageAssistant\n",
    "import random\n",
    "import jaxtyping \n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect\n",
    "Inspect (or inspect_ai) is a library written by the UK AISI in order to streamline model evaluations. Inspect provides the following in order to make writing evals easier:\n",
    "\n",
    "- It automates the creation of logging files, and stores a lot of useful information for us easily.\n",
    "\n",
    "- It automatically does exponential backing off, and provides max_connections functionality, so that we don't have to bother with this.\n",
    "\n",
    "- It provides nice \"scoring\" functions (and some of its automated solver functions also come in handy, but mostly we will write our own).\n",
    "\n",
    "- It provides a nice layout to view the results of our evals, so we don't have to look directly at model outputs (which can be very gross, as we will find out when we work with agents).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Write record_to_sample function\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴⚪⚪⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 5-10 minutes on this exercise.\n",
    "```\n",
    "\n",
    "This function will take in a single element (\"record\") of your list of json objects, and returns a Sample object. For an example of what this function might look like, look at [this section of the inspect documentation](https://inspect.ai-safety-institute.org.uk/datasets.html#field-mapping).\n",
    "\n",
    "The sample object is what the inspect_ai framework uses to deal with individual questions. The most important elements of the sample object are: \n",
    "\n",
    "- The `input` to the model. This is a `ChatMessage` list of dictionaries which contains the content which we will feed to the model during the eval consisting of:\n",
    "    - A system message\n",
    "\n",
    "    - A user message (this is the most necessary part of all)\n",
    "\n",
    "- A `target` which represents the \"correct\" answer (or `answer_matching_behavior` in our context).\n",
    "\n",
    "<details> \n",
    "<summary>Aside: Longer ChatMessage lists</summary>\n",
    "<br>\n",
    "For more complicated evals, we're able to do an arbitrary length list of ChatMessages including: \n",
    "<ul> \n",
    "<li>Multiple user messages and system messages.</li>\n",
    "&nbsp;\n",
    "<li>\"Assistant messages\" that the model will believe it has produced in response to user and system messages. However we can write these ourselves to trick the model.</li>\n",
    "&nbsp;\n",
    "<li>\"Tool call messages\" which can mimic the model's interaction with any tools that we give it. We'll learn more about tools in the agent evals section.</li>\n",
    "</ul>\n",
    "</details>\n",
    "\n",
    "Since we are constructing a multiple choice evaluation, we will also need to feed it `choices`. Then it also takes in `metadata`, which can consist of any data we might want to store related to the question in our logs (for example, if there are multiple categories of question, and we want to be able to plot results by category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1599308141.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    input = []\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "def record_to_sample(record):\n",
    "    return Sample(\n",
    "        input = [],\n",
    "        target = \n",
    "        choices = \n",
    "        metadata =\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the `json_dataset()` function to apply this function to the list of questions in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = json_dataset(\"Your/Dataset/Path/Here\" record_to_sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorers\n",
    "\n",
    "Scorers are what the inspect library uses to evaluate model output. We use them to determine whether the model's answer is the `target` answer. From the [inspect documentation](https://inspect.ai-safety-institute.org.uk/scorers.html):\n",
    "\n",
    "\n",
    "><br>\n",
    ">Scorers generally take one of the following forms:\n",
    ">\n",
    ">1. Extracting a specific answer out of a model’s completion output using a variety of heuristics.\n",
    ">\n",
    ">2. Applying a text similarity algorithm to see if the model’s completion is close to what is set out in the `target`.\n",
    ">\n",
    ">3. Using another model to assess whether the model’s completion satisfies a description of the ideal answer in `target`.\n",
    ">\n",
    ">4. Using another rubric entirely (e.g. did the model produce a valid version of a file format, etc.)\n",
    ">\n",
    "><br>\n",
    "\n",
    "Since we are writing a multiple choice benchmark, the scorer we will generally use is called `match()`. This determines whether the `target` appears at the end of the model's output (or the start, but by default it will look at the end). The code for this scorer function can be found [in the inspect_ai github](https://github.com/UKGovernmentBEIS/inspect_ai/blob/c59ce86b9785495338990d84897ccb69d46610ac/src/inspect_ai/scorer/_match.py). \n",
    "\n",
    "<details><summary>Aside: some other scorer functions:</summary>\n",
    "<br>\n",
    "Some other scorer functions that would be useful in different evals are:\n",
    "\n",
    "- `includes()` which determines whether the target appears anywhere in the model output.\n",
    "\n",
    "- `pattern()` which extracts the answer from the model output using a regular expression.\n",
    "\n",
    "- `answer()` which is a scorer for model output when the output is preceded with \"ANSWER:\".\n",
    "\n",
    "- `model_graded_qa()` which has another model assess whether the output is a correct answer based on guidance in `target`. This is especially useful for evals where the answer is more open-ended. Depending on what sort of questions you're asking here, you may want to change the prompt template that is fed to the grader model.\n",
    "<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solvers\n",
    "\n",
    "Solvers are functions which outline how the eval will be presented to the model. They can be used for:\n",
    "\n",
    "- Providing a system prompt.\n",
    " \n",
    "- Providing the evaluation question.\n",
    "\n",
    "- Prompt engineering (e.g. chain of thought, asking the model to output in certain formats, telling the model how to act when answering).\n",
    "\n",
    "- Self-critique (where the model criticises its prior answer to see if it arrives at a different conclusion).\n",
    "\n",
    "- Providing fake assistant messages to the model (useful if you want to conduct an eval in the midst of an ongoing conversation with a model).\n",
    "\n",
    "Some built in solvers which accomplish these are:\n",
    "\n",
    "- `system_message()` which adds a system message to the list of ChatMessages.\n",
    "\n",
    "- `prompt_template()` which modifies the user prompt by substituting the current prompt into the `{prompt}` placeholder witin the specified template. Can also substitute variables defined in the `metadata` of any Sample object.\n",
    "\n",
    "- `chain_of_thought()` which modifies the user prompt by substituting the current prompt into the `"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
